{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1eaa80ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ใช้ไฟล์: C:\\Users\\ACER\\OneDrive\\Desktop\\is3\\black end\\datasetis.csv\n",
      "   → อ่านสำเร็จด้วย sep=None, encoding='utf-8-sig'\n",
      "🎯 ตัวอย่าง Target: ['Balanced' 'Low_Carb' 'Low_Sodium']\n",
      "📊 สัดส่วนคลาส:\n",
      " Diet_Recommendation\n",
      "Balanced      0.426\n",
      "Low_Sodium    0.316\n",
      "Low_Carb      0.258\n",
      "Name: proportion, dtype: float64\n",
      "🔖 Target mapping: {'Balanced': 0, 'Low_Carb': 1, 'Low_Sodium': 2}\n",
      "ℹ️ ไม่พบ imbalanced-learn → ข้าม SMOTE\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "\n",
      "🏆 Best CV F1_macro: 1.0000\n",
      "🔧 Best params: {'model__n_estimators': 150, 'model__min_samples_split': 5, 'model__min_samples_leaf': 4, 'model__max_features': None, 'model__max_depth': 20}\n",
      "\n",
      "📈 Test Accuracy : 1.0\n",
      "🎯 Test F1_macro : 1.0\n",
      "\n",
      "📋 Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Balanced       1.00      1.00      1.00       128\n",
      "    Low_Carb       1.00      1.00      1.00        77\n",
      "  Low_Sodium       1.00      1.00      1.00        95\n",
      "\n",
      "    accuracy                           1.00       300\n",
      "   macro avg       1.00      1.00      1.00       300\n",
      "weighted avg       1.00      1.00      1.00       300\n",
      "\n",
      "🧩 Confusion Matrix (เลขดิบ):\n",
      " [[128   0   0]\n",
      " [  0  77   0]\n",
      " [  0   0  95]]\n",
      "\n",
      "========== WEKA-STYLE EVALUATION (Random Forest) ==========\n",
      "\n",
      "Total number of instances (after ignore): 300\n",
      "\n",
      "📊 Detailed Accuracy By Class\n",
      "     Class  Precision  Recall  F1-Score  Support\n",
      "  Balanced        1.0     1.0       1.0      128\n",
      "  Low_Carb        1.0     1.0       1.0       77\n",
      "Low_Sodium        1.0     1.0       1.0       95\n",
      "\n",
      "🧩 Confusion Matrix (rows=true, cols=predicted)\n",
      "                 pred_Balanced  pred_Low_Carb  pred_Low_Sodium\n",
      "true_Balanced              128              0                0\n",
      "true_Low_Carb                0             77                0\n",
      "true_Low_Sodium              0              0               95\n",
      "\n",
      "✅ Correctly classified instances:   300 / 300  (100.00%)\n",
      "❌ Incorrectly classified instances: 0 / 300  (0.00%)\n",
      "📦 Total number of instances:        300\n",
      "🔎 (ignore class unknown instances เปิดใช้งานแล้ว)\n",
      "\n",
      "🤝 Kappa statistic: 1.000000\n",
      "\n",
      "📐 Mean absolute error (MAE): 0.000000\n",
      "📐 Root mean squared error (RMSE): 0.000000\n",
      "📏 Relative absolute error (RAE): 0.00%\n",
      "📏 Root relative squared error (RRSE): 0.00%\n",
      "\n",
      "💾 Saved report: eval_output_rf\\summary.txt\n",
      "💾 Saved per-class metrics: eval_output_rf\\detailed_accuracy_by_class.csv\n",
      "💾 Saved confusion matrix: eval_output_rf\\confusion_matrix.csv\n",
      "\n",
      "💾 Saved: diet_recommendation_rf_model.joblib, label_encoder.joblib\n",
      "\n",
      "🧪 Example prediction: Balanced {'Balanced': 1.0, 'Low_Carb': 0.0, 'Low_Sodium': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# ==== diet_model_rf_weka_summary_full.py ====\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline as SkPipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, classification_report, confusion_matrix,\n",
    "    precision_recall_fscore_support, cohen_kappa_score\n",
    ")\n",
    "import joblib, os\n",
    "\n",
    "# ---------- 1) หาไฟล์ CSV อัตโนมัติ ----------\n",
    "ROOT_DIRS = [Path.cwd(), Path(r\"C:\\Users\\ACER\\OneDrive\\Desktop\\s3\")]\n",
    "PATTERNS = [\"*data*set*is*.csv\", \"*dataset*is*.csv\", \"*.csv\"]\n",
    "\n",
    "def find_csv(roots, patterns):\n",
    "    for root in roots:\n",
    "        if not root.exists(): continue\n",
    "        for pat in patterns:\n",
    "            for p in root.glob(pat):\n",
    "                if p.is_file(): return p.resolve()\n",
    "    return None\n",
    "\n",
    "csv_path = find_csv(ROOT_DIRS, PATTERNS)\n",
    "if not csv_path:\n",
    "    raise FileNotFoundError(\"ไม่พบไฟล์ CSV — วางไฟล์ไว้โฟลเดอร์เดียวกับสคริปต์หรือแก้ ROOT_DIRS\")\n",
    "print(f\"✅ ใช้ไฟล์: {csv_path}\")\n",
    "\n",
    "# ---------- 2) อ่านข้อมูล (กันพังหลาย encoding/sep) ----------\n",
    "read_ok, last_err = False, None\n",
    "for sep in [None, \",\", \";\", \"\\t\", \"|\"]:\n",
    "    for enc in [\"utf-8-sig\", \"utf-8\", \"cp874\", \"latin-1\"]:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path, sep=sep, encoding=enc, engine=\"python\")\n",
    "            read_ok = True\n",
    "            print(f\"   → อ่านสำเร็จด้วย sep={repr(sep)}, encoding='{enc}'\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    if read_ok: break\n",
    "if not read_ok:\n",
    "    raise RuntimeError(f\"อ่านไฟล์ไม่สำเร็จ: {last_err}\")\n",
    "\n",
    "# ---------- 3) Target / Features ----------\n",
    "POSSIBLE_TARGETS = [\"Diet_Recommendation\", \"diet_recommendation\", \"Target\"]\n",
    "target_col = next((c for c in POSSIBLE_TARGETS if c in df.columns), None)\n",
    "if not target_col:\n",
    "    raise KeyError(f\"ไม่พบคอลัมน์ Target ใน {list(df.columns)}\")\n",
    "\n",
    "y_text = df[target_col]\n",
    "X = df.drop(columns=[target_col])\n",
    "\n",
    "print(\"🎯 ตัวอย่าง Target:\", y_text.unique()[:5])\n",
    "print(\"📊 สัดส่วนคลาส:\\n\", y_text.value_counts(normalize=True).round(3))\n",
    "\n",
    "# ---------- 4) เข้ารหัสเป้าหมาย + แยกชนิดคอลัมน์ ----------\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y_text)\n",
    "categorical_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "numeric_cols     = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(\"🔖 Target mapping:\", {cls: int(i) for i, cls in enumerate(le.classes_)})\n",
    "\n",
    "# ---------- 5) Train/Test split ----------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ---------- 6) Preprocessor ----------\n",
    "try:\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)  # sklearn >=1.2\n",
    "except TypeError:\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)         # sklearn <1.2\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    [(\"cat\", ohe, categorical_cols),\n",
    "     (\"num\", StandardScaler(), numeric_cols)],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# ---------- 7) Pipeline + (optional) SMOTE ----------\n",
    "use_smote = False\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "    smote = SMOTE(random_state=42)\n",
    "    use_smote = True\n",
    "    print(\"🔄 จะใช้ SMOTE ระหว่างฝึก (พบ imbalanced-learn)\")\n",
    "except Exception:\n",
    "    smote = None\n",
    "    print(\"ℹ️ ไม่พบ imbalanced-learn → ข้าม SMOTE\")\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    random_state=42, class_weight=\"balanced\", n_jobs=-1\n",
    ")\n",
    "\n",
    "if use_smote:\n",
    "    train_pipe = ImbPipeline([(\"prep\", preprocessor), (\"smote\", smote), (\"model\", rf)])\n",
    "else:\n",
    "    train_pipe = SkPipeline([(\"prep\", preprocessor), (\"model\", rf)])\n",
    "\n",
    "# ---------- 8) RandomizedSearch + CV ----------\n",
    "param_dist = {\n",
    "    \"model__n_estimators\": [150, 250, 400, 600],\n",
    "    \"model__max_depth\": [None, 8, 12, 20],\n",
    "    \"model__min_samples_split\": [2, 5, 10],\n",
    "    \"model__min_samples_leaf\": [1, 2, 4],\n",
    "    \"model__max_features\": [\"sqrt\", \"log2\", None],\n",
    "}\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=train_pipe, param_distributions=param_dist,\n",
    "    n_iter=12, scoring=\"f1_macro\", cv=cv, n_jobs=-1, random_state=42, verbose=1\n",
    ")\n",
    "search.fit(X_train, y_train)\n",
    "print(f\"\\n🏆 Best CV F1_macro: {search.best_score_:.4f}\")\n",
    "print(\"🔧 Best params:\", search.best_params_)\n",
    "\n",
    "best_model = search.best_estimator_\n",
    "\n",
    "# ---------- 9) Inference model (ตัด SMOTE ตอนทำนาย) ----------\n",
    "if \"smote\" in best_model.named_steps:\n",
    "    prep_fitted  = best_model.named_steps[\"prep\"]\n",
    "    model_fitted = best_model.named_steps[\"model\"]\n",
    "    inference_model = SkPipeline([(\"prep\", prep_fitted), (\"model\", model_fitted)])\n",
    "else:\n",
    "    inference_model = best_model\n",
    "\n",
    "# ---------- 10) ทำนายพื้นฐาน ----------\n",
    "y_pred_num = inference_model.predict(X_test)\n",
    "y_pred_lbl = le.inverse_transform(y_pred_num)\n",
    "y_test_lbl = le.inverse_transform(y_test)\n",
    "\n",
    "print(\"\\n📈 Test Accuracy :\", round(accuracy_score(y_test_lbl, y_pred_lbl), 4))\n",
    "print(\"🎯 Test F1_macro :\", round(f1_score(y_test_lbl, y_pred_lbl, average='macro'), 4))\n",
    "print(\"\\n📋 Classification report:\\n\", classification_report(y_test_lbl, y_pred_lbl, target_names=le.classes_))\n",
    "print(\"🧩 Confusion Matrix (เลขดิบ):\\n\", confusion_matrix(y_test_lbl, y_pred_lbl, labels=le.classes_))\n",
    "\n",
    "# ---------- 11) WEKA-style Summary (ครบทุกหัวข้อที่ขอ) ----------\n",
    "def weka_like_summary(y_test_lbl, y_pred_lbl, X_test, le, inference_model,\n",
    "                      title=\"WEKA-STYLE EVALUATION\", save_dir=\"eval_output_rf\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    txt_path = Path(save_dir, \"summary.txt\")\n",
    "    percls_csv = Path(save_dir, \"detailed_accuracy_by_class.csv\")\n",
    "    cm_csv = Path(save_dir, \"confusion_matrix.csv\")\n",
    "\n",
    "    # Align index + ignore class unknown instances\n",
    "    y_true_s = pd.Series(y_test_lbl, index=X_test.index)\n",
    "    y_pred_s = pd.Series(y_pred_lbl, index=X_test.index)\n",
    "    mask = y_true_s.notna() & y_true_s.isin(le.classes_)  # ignore unknown\n",
    "    y_true = y_true_s.loc[mask].values\n",
    "    y_pred = y_pred_s.loc[mask].values\n",
    "    X_eval = X_test.loc[mask]\n",
    "    if len(y_true) == 0:\n",
    "        raise ValueError(\"ไม่มี instance สำหรับประเมินหลัง ignore class unknown instances\")\n",
    "\n",
    "    classes = list(le.classes_)\n",
    "    lines = []\n",
    "    add = lines.append\n",
    "    add(f\"\\n========== {title} ==========\\n\")\n",
    "    add(f\"Total number of instances (after ignore): {len(y_true)}\")\n",
    "\n",
    "    # 1) Detailed accuracy by class\n",
    "    prec, rec, f1, support = precision_recall_fscore_support(\n",
    "        y_true, y_pred, labels=classes, zero_division=0\n",
    "    )\n",
    "    per_class_df = pd.DataFrame({\n",
    "        \"Class\": classes, \"Precision\": prec, \"Recall\": rec, \"F1-Score\": f1, \"Support\": support.astype(int)\n",
    "    })\n",
    "    add(\"\\n📊 Detailed Accuracy By Class\")\n",
    "    add(per_class_df.to_string(index=False))\n",
    "\n",
    "    # 2) Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "    cm_df = pd.DataFrame(cm, index=[f\"true_{c}\" for c in classes],\n",
    "                            columns=[f\"pred_{c}\" for c in classes])\n",
    "    add(\"\\n🧩 Confusion Matrix (rows=true, cols=predicted)\")\n",
    "    add(cm_df.to_string())\n",
    "\n",
    "    # 3) Correct/Incorrect/Total\n",
    "    correct = int((y_true == y_pred).sum())\n",
    "    total   = int(len(y_true))\n",
    "    incorrect = total - correct\n",
    "    acc_simple = correct / total if total > 0 else float(\"nan\")\n",
    "    add(f\"\\n✅ Correctly classified instances:   {correct} / {total}  ({acc_simple*100:.2f}%)\")\n",
    "    add(f\"❌ Incorrectly classified instances: {incorrect} / {total}  ({(1-acc_simple)*100:.2f}%)\")\n",
    "    add(f\"📦 Total number of instances:        {total}\")\n",
    "    add(\"🔎 (ignore class unknown instances เปิดใช้งานแล้ว)\")\n",
    "\n",
    "    # 4) Kappa\n",
    "    y_true_num = le.transform(y_true)\n",
    "    y_pred_num = le.transform(y_pred)\n",
    "    kappa = cohen_kappa_score(y_true_num, y_pred_num)\n",
    "    add(f\"\\n🤝 Kappa statistic: {kappa:.6f}\")\n",
    "\n",
    "    # 5) MAE/RMSE (probability-based)\n",
    "    proba = inference_model.predict_proba(X_eval)  # [N, n_classes]\n",
    "    cls_to_idx = {c:i for i,c in enumerate(classes)}\n",
    "    y_true_idx = np.array([cls_to_idx[c] for c in y_true])\n",
    "    y_onehot = np.eye(len(classes))[y_true_idx]\n",
    "    abs_err = np.abs(y_onehot - proba).sum(axis=1) / 2.0\n",
    "    mae = abs_err.mean()\n",
    "    sq_err = ((y_onehot - proba) ** 2).sum(axis=1) / 2.0\n",
    "    rmse = np.sqrt(sq_err.mean())\n",
    "    add(f\"\\n📐 Mean absolute error (MAE): {mae:.6f}\")\n",
    "    add(f\"📐 Root mean squared error (RMSE): {rmse:.6f}\")\n",
    "\n",
    "    # 6) RAE/RRSE เทียบ baseline (prior distribution)\n",
    "    prior_counts = pd.Series(y_true).value_counts().reindex(classes, fill_value=0).values\n",
    "    prior_dist = prior_counts / prior_counts.sum()\n",
    "    abs_err_base = np.abs(y_onehot - prior_dist).sum(axis=1) / 2.0\n",
    "    mae_base = abs_err_base.mean()\n",
    "    sq_err_base = ((y_onehot - prior_dist) ** 2).sum(axis=1) / 2.0\n",
    "    rmse_base = np.sqrt(sq_err_base.mean())\n",
    "    rae  = (mae / mae_base) * 100.0 if mae_base > 0 else float(\"inf\")\n",
    "    rrse = (rmse / rmse_base) * 100.0 if rmse_base > 0 else float(\"inf\")\n",
    "    add(f\"📏 Relative absolute error (RAE): {rae:.2f}%\")\n",
    "    add(f\"📏 Root relative squared error (RRSE): {rrse:.2f}%\")\n",
    "\n",
    "    # print ครบ\n",
    "    print(\"\\n\".join(lines))\n",
    "    # save รายงาน/ตาราง\n",
    "    with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(lines))\n",
    "    per_class_df.to_csv(percls_csv, index=False, encoding=\"utf-8-sig\")\n",
    "    cm_df.to_csv(cm_csv, encoding=\"utf-8-sig\")\n",
    "    print(f\"\\n💾 Saved report: {txt_path}\")\n",
    "    print(f\"💾 Saved per-class metrics: {percls_csv}\")\n",
    "    print(f\"💾 Saved confusion matrix: {cm_csv}\")\n",
    "\n",
    "    return {\n",
    "        \"per_class\": per_class_df, \"confusion_matrix\": cm_df,\n",
    "        \"correct\": correct, \"incorrect\": incorrect, \"total\": total,\n",
    "        \"kappa\": kappa, \"mae\": mae, \"rmse\": rmse, \"rae_pct\": rae, \"rrse_pct\": rrse\n",
    "    }\n",
    "\n",
    "# เรียกสรุปผลแบบ WEKA‑style (ครบทั้งหมดที่ขอ)\n",
    "_ = weka_like_summary(\n",
    "    y_test_lbl=y_test_lbl,\n",
    "    y_pred_lbl=y_pred_lbl,\n",
    "    X_test=X_test,\n",
    "    le=le,\n",
    "    inference_model=inference_model,\n",
    "    title=\"WEKA-STYLE EVALUATION (Random Forest)\"\n",
    ")\n",
    "\n",
    "# ---------- 12) เซฟโมเดล + LabelEncoder ----------\n",
    "joblib.dump(inference_model, \"diet_recommendation_rf_model.joblib\")\n",
    "joblib.dump(le, \"label_encoder.joblib\")\n",
    "print(\"\\n💾 Saved: diet_recommendation_rf_model.joblib, label_encoder.joblib\")\n",
    "\n",
    "# ---------- 13) ฟังก์ชันทำนาย 1 รายการ ----------\n",
    "def predict_one(sample_dict: dict):\n",
    "    sample_df = pd.DataFrame([sample_dict]).reindex(columns=X.columns)\n",
    "    for col in numeric_cols:\n",
    "        sample_df[col] = pd.to_numeric(sample_df[col], errors=\"coerce\")\n",
    "    for col in categorical_cols:\n",
    "        sample_df[col] = sample_df[col].astype(str)\n",
    "    pred_num = inference_model.predict(sample_df)[0]\n",
    "    pred_lbl = le.inverse_transform([pred_num])[0]\n",
    "    proba = inference_model.predict_proba(sample_df)[0]\n",
    "    return pred_lbl, dict(zip(le.classes_, map(float, proba)))\n",
    "\n",
    "# ---- Demo ----\n",
    "ex = X.iloc[0].to_dict()\n",
    "lbl, pro = predict_one(ex)\n",
    "print(\"\\n🧪 Example prediction:\", lbl, pro)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
